{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2GTGHKFuspN8Twn8cs8AY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arumishra/Assignment-Codes/blob/main/complexnetworks2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32ijQjqF2K1T",
        "outputId": "bfb67ef9-2def-48d2-a4c5-9f199d5d9a6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted files: ['README', 'wisconsin.cites', 'washington.content', 'wisconsin.content', 'texas.cites', 'cornell.content', 'washington.cites', 'texas.content', 'cornell.cites']\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/webkb.zip\"  # Path to the ZIP file\n",
        "extract_path = \"/content\"  # Extract directly to /content\n",
        "\n",
        "# Extract the dataset\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# Set the correct data directory path\n",
        "data_directory = \"/content/webkb\"  # Now this should directly contain the files\n",
        "\n",
        "# List extracted files to verify\n",
        "print(\"Extracted files:\", os.listdir(data_directory))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install torch-scatter, torch-sparse, torch-cluster, and torch-spline-conv (PyG dependencies)\n",
        "# Select appropriate versions based on your torch version (below assumes torch 2.0.0+ and CUDA 11.7)\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
        "!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
        "\n",
        "# Now install PyTorch Geometric\n",
        "!pip install torch-geometric\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bst9TTTi2Llm",
        "outputId": "2434d178-542e-454b-d826-dbb63cfb67ca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_scatter-2.1.2%2Bpt20cpu-cp311-cp311-linux_x86_64.whl (494 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m494.0/494.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt20cpu\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_sparse-0.6.18%2Bpt20cpu-cp311-cp311-linux_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.14.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt20cpu\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_cluster-1.6.3%2Bpt20cpu-cp311-cp311-linux_x86_64.whl (750 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m750.9/750.9 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-cluster) (1.14.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-cluster) (2.0.2)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3+pt20cpu\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_spline_conv-1.2.2%2Bpt20cpu-cp311-cp311-linux_x86_64.whl (208 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m208.1/208.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.2+pt20cpu\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.3.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define paths\n",
        "data_directory = \"/content/webkb\"\n",
        "output_cites = \"/content/webkb_combined/combined.cites\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(\"/content/webkb_combined\", exist_ok=True)\n",
        "\n",
        "# Merge .cites files\n",
        "with open(output_cites, \"w\") as outfile:\n",
        "    for file in os.listdir(data_directory):\n",
        "        if file.endswith(\".cites\"):\n",
        "            file_path = os.path.join(data_directory, file)\n",
        "            print(f\"üîÑ Merging {file_path}\")\n",
        "            with open(file_path, \"r\") as infile:\n",
        "                lines = infile.readlines()\n",
        "                if lines:\n",
        "                    print(f\"‚úÖ {file} has {len(lines)} edges\")\n",
        "                    outfile.writelines(lines)\n",
        "                else:\n",
        "                    print(f\"‚ö†Ô∏è {file} is empty!\")\n",
        "\n",
        "print(\"‚úÖ Merging .cites files complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OizJIQna2Lnt",
        "outputId": "0abc0bcf-076e-4b44-a520-a3333c3f76d9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Merging /content/webkb/wisconsin.cites\n",
            "‚úÖ wisconsin.cites has 530 edges\n",
            "üîÑ Merging /content/webkb/texas.cites\n",
            "‚úÖ texas.cites has 328 edges\n",
            "üîÑ Merging /content/webkb/washington.cites\n",
            "‚úÖ washington.cites has 446 edges\n",
            "üîÑ Merging /content/webkb/cornell.cites\n",
            "‚úÖ cornell.cites has 304 edges\n",
            "‚úÖ Merging .cites files complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define output file path\n",
        "output_content = \"/content/webkb_combined/combined.content\"\n",
        "\n",
        "# Merge .content files\n",
        "with open(output_content, \"w\") as outfile:\n",
        "    for file in os.listdir(data_directory):\n",
        "        if file.endswith(\".content\"):\n",
        "            file_path = os.path.join(data_directory, file)\n",
        "            print(f\"üîÑ Merging {file_path}\")\n",
        "            with open(file_path, \"r\") as infile:\n",
        "                lines = infile.readlines()\n",
        "                if lines:\n",
        "                    print(f\"‚úÖ {file} has {len(lines)} nodes\")\n",
        "                    outfile.writelines(lines)\n",
        "                else:\n",
        "                    print(f\"‚ö†Ô∏è {file} is empty!\")\n",
        "\n",
        "print(\"‚úÖ Merging .content files complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEMsM8qN2LqP",
        "outputId": "1de13eec-e058-434a-c1ba-ce1a5651eae8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Merging /content/webkb/washington.content\n",
            "‚úÖ washington.content has 230 nodes\n",
            "üîÑ Merging /content/webkb/wisconsin.content\n",
            "‚úÖ wisconsin.content has 265 nodes\n",
            "üîÑ Merging /content/webkb/cornell.content\n",
            "‚úÖ cornell.content has 195 nodes\n",
            "üîÑ Merging /content/webkb/texas.content\n",
            "‚úÖ texas.content has 187 nodes\n",
            "‚úÖ Merging .content files complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"‚úÖ Size of combined.cites:\", os.path.getsize(output_cites), \"bytes\")\n",
        "print(\"‚úÖ Size of combined.content:\", os.path.getsize(output_content), \"bytes\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD8SKVTY2Lso",
        "outputId": "16ee912e-d8bf-4771-f1c2-da35b51c425a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Size of combined.cites: 137522 bytes\n",
            "‚úÖ Size of combined.content: 3034502 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GINConv\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "# Load WebKB Dataset\n",
        "def load_webkb_dataset(content_path, cites_path):\n",
        "    paper_ids = []\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    with open(content_path, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            paper_ids.append(parts[0])\n",
        "            features.append([int(x) for x in parts[1:-1]])\n",
        "            labels.append(parts[-1])\n",
        "\n",
        "    id_to_idx = {paper_id: idx for idx, paper_id in enumerate(paper_ids)}\n",
        "    x = torch.tensor(features, dtype=torch.float)\n",
        "    label_set = sorted(set(labels))\n",
        "    label_to_idx = {label: i for i, label in enumerate(label_set)}\n",
        "    y = torch.tensor([label_to_idx[label] for label in labels], dtype=torch.long)\n",
        "\n",
        "    edge_index = []\n",
        "    with open(cites_path, 'r') as f:\n",
        "        for line in f:\n",
        "            src, dst = line.strip().split()\n",
        "            if src in id_to_idx and dst in id_to_idx:\n",
        "                src_idx = id_to_idx[src]\n",
        "                dst_idx = id_to_idx[dst]\n",
        "                edge_index.append([src_idx, dst_idx])\n",
        "                edge_index.append([dst_idx, src_idx])\n",
        "\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "    data = Data(x=x, edge_index=edge_index, y=y)\n",
        "    return data, label_to_idx\n",
        "\n",
        "# Create Train/Test Masks\n",
        "def split_train_test(data, train_ratio=0.7, seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    indices = np.arange(data.num_nodes)\n",
        "    labels = data.y.numpy()\n",
        "    train_idx, test_idx = train_test_split(indices, train_size=train_ratio, stratify=labels, random_state=seed)\n",
        "\n",
        "    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "    test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "    train_mask[train_idx] = True\n",
        "    test_mask[test_idx] = True\n",
        "    data.train_mask = train_mask\n",
        "    data.test_mask = test_mask\n",
        "    return data\n",
        "\n",
        "# GIN Model\n",
        "class GIN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.5):\n",
        "        super(GIN, self).__init__()\n",
        "        nn1 = nn.Sequential(nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim))\n",
        "        self.conv1 = GINConv(nn1)\n",
        "        nn2 = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim))\n",
        "        self.conv2 = GINConv(nn2)\n",
        "        self.classifier = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        out = self.classifier(x)\n",
        "        return out\n",
        "\n",
        "# Train and Evaluate\n",
        "def train(model, data, optimizer, criterion):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def test(model, data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data)\n",
        "        pred = out.argmax(dim=1)\n",
        "        y_true = data.y[data.test_mask].cpu()\n",
        "        y_pred = pred[data.test_mask].cpu()\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        prec, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n",
        "    return acc, prec, recall, f1\n",
        "\n",
        "# Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    content_path = \"webkb_combined/combined.content\"\n",
        "    cites_path = \"webkb_combined/combined.cites\"\n",
        "\n",
        "    data, label_map = load_webkb_dataset(content_path, cites_path)\n",
        "    data = split_train_test(data)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = GIN(input_dim=data.num_node_features, hidden_dim=16, output_dim=len(label_map)).to(device)\n",
        "    data = data.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"Training GIN model...\\n\")\n",
        "    for epoch in range(1, 201):\n",
        "        loss = train(model, data, optimizer, criterion)\n",
        "        if epoch % 20 == 0:\n",
        "            acc, prec, rec, f1 = test(model, data)\n",
        "            print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Acc: {acc:.4f} | F1: {f1:.4f}\")\n",
        "\n",
        "    # Final Evaluation\n",
        "    acc, prec, rec, f1 = test(model, data)\n",
        "    print(\"\\n--- Final Test Metrics ---\")\n",
        "    print(f\"Accuracy:  {acc:.4f}\")\n",
        "    print(f\"Precision: {prec:.4f}\")\n",
        "    print(f\"Recall:    {rec:.4f}\")\n",
        "    print(f\"F1 Score:  {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6gnfXqj2Lu5",
        "outputId": "06be8d18-fbba-4cd8-b0a3-7606ae135b07"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_scatter/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_cluster/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_spline_conv/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_sparse/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training GIN model...\n",
            "\n",
            "Epoch 020 | Loss: 1.3846 | Acc: 0.4735 | F1: 0.1285\n",
            "Epoch 040 | Loss: 1.2998 | Acc: 0.4735 | F1: 0.1285\n",
            "Epoch 060 | Loss: 1.2783 | Acc: 0.4735 | F1: 0.1285\n",
            "Epoch 080 | Loss: 1.2648 | Acc: 0.4735 | F1: 0.1285\n",
            "Epoch 100 | Loss: 1.2395 | Acc: 0.4583 | F1: 0.2149\n",
            "Epoch 120 | Loss: 1.2611 | Acc: 0.5076 | F1: 0.2355\n",
            "Epoch 140 | Loss: 1.1851 | Acc: 0.5114 | F1: 0.2374\n",
            "Epoch 160 | Loss: 1.1275 | Acc: 0.6023 | F1: 0.2794\n",
            "Epoch 180 | Loss: 1.0647 | Acc: 0.5985 | F1: 0.2782\n",
            "Epoch 200 | Loss: 0.9714 | Acc: 0.6023 | F1: 0.2820\n",
            "\n",
            "--- Final Test Metrics ---\n",
            "Accuracy:  0.6023\n",
            "Precision: 0.2514\n",
            "Recall:    0.3253\n",
            "F1 Score:  0.2820\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CqHq2WO62LxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GAT\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load Dataset\n",
        "def load_webkb_dataset(content_path, cites_path):\n",
        "    paper_ids = []\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    with open(content_path, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            paper_ids.append(parts[0])\n",
        "            features.append([int(x) for x in parts[1:-1]])\n",
        "            labels.append(parts[-1])\n",
        "\n",
        "    id_to_idx = {paper_id: idx for idx, paper_id in enumerate(paper_ids)}\n",
        "    x = torch.tensor(features, dtype=torch.float)\n",
        "    label_set = sorted(set(labels))\n",
        "    label_to_idx = {label: i for i, label in enumerate(label_set)}\n",
        "    y = torch.tensor([label_to_idx[label] for label in labels], dtype=torch.long)\n",
        "\n",
        "    edge_index = []\n",
        "    with open(cites_path, 'r') as f:\n",
        "        for line in f:\n",
        "            src, dst = line.strip().split()\n",
        "            if src in id_to_idx and dst in id_to_idx:\n",
        "                src_idx = id_to_idx[src]\n",
        "                dst_idx = id_to_idx[dst]\n",
        "                edge_index.append([src_idx, dst_idx])\n",
        "                edge_index.append([dst_idx, src_idx])\n",
        "\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "    data = Data(x=x, edge_index=edge_index, y=y)\n",
        "    return data, label_to_idx\n",
        "\n",
        "# 2. Create train/test split\n",
        "def split_train_test(data, train_ratio=0.7, seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    indices = np.arange(data.num_nodes)\n",
        "    labels = data.y.numpy()\n",
        "    train_idx, test_idx = train_test_split(indices, train_size=train_ratio, stratify=labels, random_state=seed)\n",
        "\n",
        "    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "    test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "    train_mask[train_idx] = True\n",
        "    test_mask[test_idx] = True\n",
        "    data.train_mask = train_mask\n",
        "    data.test_mask = test_mask\n",
        "    return data\n",
        "\n",
        "# 3. Define GAT model\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, heads=8, dropout=0.5):\n",
        "        super(GAT, self).__init__()\n",
        "        self.gat1 = GATConv(input_dim, hidden_dim, heads=heads, dropout=dropout)\n",
        "        self.gat2 = GATConv(hidden_dim * heads, output_dim, heads=1, concat=False, dropout=dropout)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.gat1(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.gat2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# 4. Train and Evaluate\n",
        "def train(model, data, optimizer, criterion):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def test(model, data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data)\n",
        "        pred = out.argmax(dim=1)\n",
        "        y_true = data.y[data.test_mask].cpu()\n",
        "        y_pred = pred[data.test_mask].cpu()\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        prec, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n",
        "    return acc, prec, recall, f1\n",
        "\n",
        "# 5. Run everything\n",
        "if __name__ == \"__main__\":\n",
        "    content_path = \"webkb_combined/combined.content\"\n",
        "    cites_path = \"webkb_combined/combined.cites\"\n",
        "\n",
        "    data, label_map = load_webkb_dataset(content_path, cites_path)\n",
        "    data = split_train_test(data)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = GAT(input_dim=data.num_node_features, hidden_dim=16, output_dim=len(label_map)).to(device)\n",
        "    data = data.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"Training GAT model...\\n\")\n",
        "    for epoch in range(1, 201):\n",
        "        loss = train(model, data, optimizer, criterion)\n",
        "        if epoch % 20 == 0:\n",
        "            acc, prec, rec, f1 = test(model, data)\n",
        "            print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Acc: {acc:.4f} | F1: {f1:.4f}\")\n",
        "\n",
        "    # Final evaluation\n",
        "    acc, prec, rec, f1 = test(model, data)\n",
        "    print(\"\\n--- Final Test Metrics ---\")\n",
        "    print(f\"Accuracy:  {acc:.4f}\")\n",
        "    print(f\"Precision: {prec:.4f}\")\n",
        "    print(f\"Recall:    {rec:.4f}\")\n",
        "    print(f\"F1 Score:  {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZY-CsqH2Lzs",
        "outputId": "ca5d1a4b-a8ea-40dd-e792-6d5f136eb7cb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training GAT model...\n",
            "\n",
            "Epoch 020 | Loss: 1.3026 | Acc: 0.5455 | F1: 0.2861\n",
            "Epoch 040 | Loss: 1.1143 | Acc: 0.5795 | F1: 0.3372\n",
            "Epoch 060 | Loss: 0.9934 | Acc: 0.5833 | F1: 0.3595\n",
            "Epoch 080 | Loss: 0.9040 | Acc: 0.5682 | F1: 0.3847\n",
            "Epoch 100 | Loss: 0.8444 | Acc: 0.5947 | F1: 0.4083\n",
            "Epoch 120 | Loss: 0.7996 | Acc: 0.5644 | F1: 0.3625\n",
            "Epoch 140 | Loss: 0.7808 | Acc: 0.5758 | F1: 0.3711\n",
            "Epoch 160 | Loss: 0.8485 | Acc: 0.5758 | F1: 0.3630\n",
            "Epoch 180 | Loss: 0.7597 | Acc: 0.5568 | F1: 0.3544\n",
            "Epoch 200 | Loss: 0.7111 | Acc: 0.5644 | F1: 0.3642\n",
            "\n",
            "--- Final Test Metrics ---\n",
            "Accuracy:  0.5644\n",
            "Precision: 0.3923\n",
            "Recall:    0.3628\n",
            "F1 Score:  0.3642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#R-GCN"
      ],
      "metadata": {
        "id": "avYaaMUF2L2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import RGCNConv\n"
      ],
      "metadata": {
        "id": "Xxj3Grqg3T-f"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GINConv\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "# Load WebKB Dataset\n",
        "def load_webkb_dataset(content_path, cites_path):\n",
        "    paper_ids = []\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    with open(content_path, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            paper_ids.append(parts[0])\n",
        "            features.append([int(x) for x in parts[1:-1]])\n",
        "            labels.append(parts[-1])\n",
        "\n",
        "    id_to_idx = {paper_id: idx for idx, paper_id in enumerate(paper_ids)}\n",
        "    x = torch.tensor(features, dtype=torch.float)\n",
        "    label_set = sorted(set(labels))\n",
        "    label_to_idx = {label: i for i, label in enumerate(label_set)}\n",
        "    y = torch.tensor([label_to_idx[label] for label in labels], dtype=torch.long)\n",
        "\n",
        "    edge_index = []\n",
        "    with open(cites_path, 'r') as f:\n",
        "        for line in f:\n",
        "            src, dst = line.strip().split()\n",
        "            if src in id_to_idx and dst in id_to_idx:\n",
        "                src_idx = id_to_idx[src]\n",
        "                dst_idx = id_to_idx[dst]\n",
        "                edge_index.append([src_idx, dst_idx])\n",
        "                edge_index.append([dst_idx, src_idx])\n",
        "\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "    #data = Data(x=x, edge_index=edge_index, y=y)\n",
        "    edge_type = torch.zeros(edge_index.size(1), dtype=torch.long)  # All edges have relation type 0\n",
        "    data = Data(x=x, edge_index=edge_index, edge_type=edge_type, y=y)\n",
        "\n",
        "    return data, label_to_idx\n",
        "\n",
        "# Create Train/Test Masks\n",
        "def split_train_test(data, train_ratio=0.7, seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    indices = np.arange(data.num_nodes)\n",
        "    labels = data.y.numpy()\n",
        "    train_idx, test_idx = train_test_split(indices, train_size=train_ratio, stratify=labels, random_state=seed)\n",
        "\n",
        "    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "    test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "    train_mask[train_idx] = True\n",
        "    test_mask[test_idx] = True\n",
        "    data.train_mask = train_mask\n",
        "    data.test_mask = test_mask\n",
        "    return data\n",
        "\n",
        "class RGCN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_relations=1, dropout=0.5):\n",
        "        super(RGCN, self).__init__()\n",
        "        self.conv1 = RGCNConv(input_dim, hidden_dim, num_relations)\n",
        "        self.conv2 = RGCNConv(hidden_dim, hidden_dim, num_relations)\n",
        "        self.classifier = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_type = data.x, data.edge_index, data.edge_type\n",
        "        x = self.conv1(x, edge_index, edge_type)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index, edge_type)\n",
        "        x = F.relu(x)\n",
        "        out = self.classifier(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Train and Evaluate\n",
        "def train(model, data, optimizer, criterion):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def test(model, data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data)\n",
        "        pred = out.argmax(dim=1)\n",
        "        y_true = data.y[data.test_mask].cpu()\n",
        "        y_pred = pred[data.test_mask].cpu()\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        prec, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n",
        "    return acc, prec, recall, f1\n",
        "\n",
        "# Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    content_path = \"webkb_combined/combined.content\"\n",
        "    cites_path = \"webkb_combined/combined.cites\"\n",
        "\n",
        "    data, label_map = load_webkb_dataset(content_path, cites_path)\n",
        "    data = split_train_test(data)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    #model = GIN(input_dim=data.num_node_features, hidden_dim=16, output_dim=len(label_map)).to(device)\n",
        "    model = RGCN(input_dim=data.num_node_features, hidden_dim=16, output_dim=len(label_map), num_relations=1).to(device)\n",
        "\n",
        "    data = data.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"Training R-GCN model...\\n\")\n",
        "    for epoch in range(1, 201):\n",
        "        loss = train(model, data, optimizer, criterion)\n",
        "        if epoch % 20 == 0:\n",
        "            acc, prec, rec, f1 = test(model, data)\n",
        "            print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Acc: {acc:.4f} | F1: {f1:.4f}\")\n",
        "\n",
        "    # Final Evaluation\n",
        "    acc, prec, rec, f1 = test(model, data)\n",
        "    print(\"\\n--- Final Test Metrics ---\")\n",
        "    print(f\"Accuracy:  {acc:.4f}\")\n",
        "    print(f\"Precision: {prec:.4f}\")\n",
        "    print(f\"Recall:    {rec:.4f}\")\n",
        "    print(f\"F1 Score:  {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1raX4Oh2L4t",
        "outputId": "6437e279-6df0-48a3-feec-f55dcf2ff6b9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training GIN model...\n",
            "\n",
            "Epoch 020 | Loss: 0.5277 | Acc: 0.7803 | F1: 0.5453\n",
            "Epoch 040 | Loss: 0.1891 | Acc: 0.8258 | F1: 0.6559\n",
            "Epoch 060 | Loss: 0.1370 | Acc: 0.8258 | F1: 0.6653\n",
            "Epoch 080 | Loss: 0.1219 | Acc: 0.8144 | F1: 0.6530\n",
            "Epoch 100 | Loss: 0.1367 | Acc: 0.8295 | F1: 0.6843\n",
            "Epoch 120 | Loss: 0.0885 | Acc: 0.8295 | F1: 0.6757\n",
            "Epoch 140 | Loss: 0.0866 | Acc: 0.8258 | F1: 0.6718\n",
            "Epoch 160 | Loss: 0.0901 | Acc: 0.8144 | F1: 0.6574\n",
            "Epoch 180 | Loss: 0.0851 | Acc: 0.8068 | F1: 0.6207\n",
            "Epoch 200 | Loss: 0.0540 | Acc: 0.8030 | F1: 0.6240\n",
            "\n",
            "--- Final Test Metrics ---\n",
            "Accuracy:  0.8030\n",
            "Precision: 0.6997\n",
            "Recall:    0.5994\n",
            "F1 Score:  0.6240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1eFR-Zch2L7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "92JbEECT2L99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W1mWyHwJ2MAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KnnMeEMK2MDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DFwZyg1Y2MFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gu5tsLRB2MJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wsfZ7RYI2MMn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}