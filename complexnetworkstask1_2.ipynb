{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8uKmPY18m6aHdqld+gc+M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arumishra/Assignment-Codes/blob/main/complexnetworkstask1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/webkb.zip\"  # Path to the ZIP file\n",
        "extract_path = \"/content\"  # Extract directly to /content\n",
        "\n",
        "# Extract the dataset\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# Set the correct data directory path\n",
        "data_directory = \"/content/webkb\"  # Now this should directly contain the files\n",
        "\n",
        "# List extracted files to verify\n",
        "print(\"Extracted files:\", os.listdir(data_directory))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl6l-CwU3JUq",
        "outputId": "b4358a75-3449-468f-e09a-0db290c59489"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted files: ['wisconsin.cites', 'cornell.cites', 'texas.cites', 'washington.content', 'wisconsin.content', 'texas.content', 'cornell.content', 'README', 'washington.cites', 'webkb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define paths\n",
        "data_directory = \"/content/webkb\"\n",
        "output_cites = \"/content/webkb_combined/combined.cites\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(\"/content/webkb_combined\", exist_ok=True)\n",
        "\n",
        "# Merge .cites files\n",
        "with open(output_cites, \"w\") as outfile:\n",
        "    for file in os.listdir(data_directory):\n",
        "        if file.endswith(\".cites\"):\n",
        "            file_path = os.path.join(data_directory, file)\n",
        "            print(f\"🔄 Merging {file_path}\")\n",
        "            with open(file_path, \"r\") as infile:\n",
        "                lines = infile.readlines()\n",
        "                if lines:\n",
        "                    print(f\"✅ {file} has {len(lines)} edges\")\n",
        "                    outfile.writelines(lines)\n",
        "                else:\n",
        "                    print(f\"⚠️ {file} is empty!\")\n",
        "\n",
        "print(\"✅ Merging .cites files complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iTK4Om73JXF",
        "outputId": "fa9ed918-4f07-4b17-c8f2-adc0397e394b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Merging /content/webkb/wisconsin.cites\n",
            "✅ wisconsin.cites has 530 edges\n",
            "🔄 Merging /content/webkb/cornell.cites\n",
            "✅ cornell.cites has 304 edges\n",
            "🔄 Merging /content/webkb/texas.cites\n",
            "✅ texas.cites has 328 edges\n",
            "🔄 Merging /content/webkb/washington.cites\n",
            "✅ washington.cites has 446 edges\n",
            "✅ Merging .cites files complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define output file path\n",
        "output_content = \"/content/webkb_combined/combined.content\"\n",
        "\n",
        "# Merge .content files\n",
        "with open(output_content, \"w\") as outfile:\n",
        "    for file in os.listdir(data_directory):\n",
        "        if file.endswith(\".content\"):\n",
        "            file_path = os.path.join(data_directory, file)\n",
        "            print(f\"🔄 Merging {file_path}\")\n",
        "            with open(file_path, \"r\") as infile:\n",
        "                lines = infile.readlines()\n",
        "                if lines:\n",
        "                    print(f\"✅ {file} has {len(lines)} nodes\")\n",
        "                    outfile.writelines(lines)\n",
        "                else:\n",
        "                    print(f\"⚠️ {file} is empty!\")\n",
        "\n",
        "print(\"✅ Merging .content files complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5E99-6KV3JZo",
        "outputId": "b4643c66-ba0c-4b2d-d1e4-0c8792e52dd6"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Merging /content/webkb/washington.content\n",
            "✅ washington.content has 230 nodes\n",
            "🔄 Merging /content/webkb/wisconsin.content\n",
            "✅ wisconsin.content has 265 nodes\n",
            "🔄 Merging /content/webkb/texas.content\n",
            "✅ texas.content has 187 nodes\n",
            "🔄 Merging /content/webkb/cornell.content\n",
            "✅ cornell.content has 195 nodes\n",
            "✅ Merging .content files complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"✅ Size of combined.cites:\", os.path.getsize(output_cites), \"bytes\")\n",
        "print(\"✅ Size of combined.content:\", os.path.getsize(output_content), \"bytes\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Y67Z4Jv3JcK",
        "outputId": "7de247df-5b05-4b7c-fd32-2f0ffb57cead"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Size of combined.cites: 137522 bytes\n",
            "✅ Size of combined.content: 3034502 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def split_cites_file(input_file, train_file, test_file, split_ratio=0.7):\n",
        "    with open(input_file, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    random.shuffle(lines)\n",
        "    split_idx = int(len(lines) * split_ratio)\n",
        "\n",
        "    with open(train_file, \"w\") as f:\n",
        "        f.writelines(lines[:split_idx])\n",
        "\n",
        "    with open(test_file, \"w\") as f:\n",
        "        f.writelines(lines[split_idx:])\n",
        "\n",
        "# Run the function\n",
        "split_cites_file(\"webkb_combined/combined.cites\",\n",
        "                 \"webkb_combined/combined_train.cites\",\n",
        "                 \"webkb_combined/combined_test.cites\")\n"
      ],
      "metadata": {
        "id": "LoR4E4Am3Jez"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def load_webkb_data(data_dir, train_cites_file):\n",
        "    graph = defaultdict(list)\n",
        "    labels = {}\n",
        "\n",
        "    # Load node labels from combined.content\n",
        "    with open(os.path.join(data_dir, \"combined.content\"), \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            webpage_id, class_label = parts[0], parts[-1]\n",
        "            labels[webpage_id] = class_label\n",
        "            graph[webpage_id] = []  # Ensure every node appears in the graph\n",
        "\n",
        "    # Load edges from combined_train.cites\n",
        "    with open(train_cites_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            cited, citing = line.strip().split()\n",
        "            graph[citing].append(cited)  # Directed edge: citing → cited\n",
        "\n",
        "    return graph, labels\n",
        "\n",
        "# Load train graph\n",
        "graph, labels = load_webkb_data(\"webkb_combined\", \"webkb_combined/combined_train.cites\")\n",
        "# Load test graph\n",
        "test_graph, test_labels = load_webkb_data(\"webkb_combined\", \"webkb_combined/combined_test.cites\")\n",
        "\n",
        "\n",
        "print(\"✅ Data Loaded: Sample Graph:\", dict(list(graph.items())[:5]))\n",
        "print(\"✅ Labels Loaded: Sample Labels:\", dict(list(labels.items())[:5]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oI0MS3XT3Jhs",
        "outputId": "4bdfd5e9-fd8b-40bf-986a-6c915816521d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Data Loaded: Sample Graph: {'http://metacrawler.cs.washington.edu:8080': ['http://metacrawler.cs.washington.edu:8080', 'http://www.cs.washington.edu/homes/etzioni'], 'http://www.cs.washington.edu': [], 'http://www.cs.washington.edu/education/courses/135': ['http://www.cs.washington.edu/education/courses/135'], 'http://www.cs.washington.edu/education/courses/142/95a': ['http://www.cs.washington.edu/education/courses/142/95a'], 'http://www.cs.washington.edu/education/courses/142/currentqtr': ['http://www.cs.washington.edu/education/courses/142/95a', 'http://www.cs.washington.edu/homes/dickey', 'http://www.cs.washington.edu/education/courses/142/currentqtr']}\n",
            "✅ Labels Loaded: Sample Labels: {'http://metacrawler.cs.washington.edu:8080': 'project', 'http://www.cs.washington.edu': 'course', 'http://www.cs.washington.edu/education/courses/135': 'course', 'http://www.cs.washington.edu/education/courses/142/95a': 'course', 'http://www.cs.washington.edu/education/courses/142/currentqtr': 'course'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_split_ratio():\n",
        "    original_file = \"webkb_combined/combined.cites\"\n",
        "    train_file = \"webkb_combined/combined_train.cites\"\n",
        "    test_file = \"webkb_combined/combined_test.cites\"\n",
        "\n",
        "    # Count edges in each file\n",
        "    total_edges = sum(1 for _ in open(original_file, \"r\"))\n",
        "    train_edges = sum(1 for _ in open(train_file, \"r\"))\n",
        "    test_edges = sum(1 for _ in open(test_file, \"r\"))\n",
        "\n",
        "    # Compute the split ratio\n",
        "    train_ratio = train_edges / total_edges\n",
        "    test_ratio = test_edges / total_edges\n",
        "\n",
        "    # Print verification\n",
        "    print(f\"✅ Total Edges: {total_edges}\")\n",
        "    print(f\"✅ Training Edges: {train_edges} ({train_ratio:.2%})\")\n",
        "    print(f\"✅ Testing Edges: {test_edges} ({test_ratio:.2%})\")\n",
        "\n",
        "check_split_ratio()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sREtZwuG2dl",
        "outputId": "8f534e26-e12a-427e-9d80-08d9abb18b83"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Total Edges: 1608\n",
            "✅ Training Edges: 1125 (69.96%)\n",
            "✅ Testing Edges: 483 (30.04%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWQvYFHa3JnV",
        "outputId": "7da393d7-4ad7-4f6d-eb94-6aa6bfd04b5e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Random Walks Generated! Sample: [['http://www.cs.utexas.edu/users/novak/cs375.html'], ['http://simon.cs.cornell.edu/info/courses/current/cs401'], ['http://www.cs.utexas.edu/users/rdb/cs372', 'http://www.cs.utexas.edu/users/rdb', 'http://www.cs.utexas.edu/users/less', 'http://www.cs.utexas.edu/users/rdb', 'http://www.cs.utexas.edu/users/rdb', 'http://www.cs.utexas.edu/users/rdb', 'http://www.cs.utexas.edu/users/rdb', 'http://www.cs.utexas.edu/users/less', 'http://www.cs.utexas.edu/users/rdb', 'http://www.cs.utexas.edu/users/rdb']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3y759kGaCUuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M0TrCTkeCUyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class Node2VecRandomWalker:\n",
        "    def __init__(self, graph, p=1.0, q=1.0, walk_length=10, num_walks=5):\n",
        "        \"\"\"\n",
        "        Initializes the Node2Vec random walker.\n",
        "\n",
        "        Args:\n",
        "            graph (dict): Adjacency list representation of the graph.\n",
        "            p (float): Return parameter (higher means staying closer to the start node).\n",
        "            q (float): In-out parameter (higher means exploring more).\n",
        "            walk_length (int): Number of steps per walk.\n",
        "            num_walks (int): Number of random walks per node.\n",
        "        \"\"\"\n",
        "        self.graph = graph\n",
        "        self.p = p\n",
        "        self.q = q\n",
        "        self.walk_length = walk_length\n",
        "        self.num_walks = num_walks\n",
        "\n",
        "    def random_walk(self, start_node):\n",
        "        \"\"\"\n",
        "        Performs a biased random walk starting from the given node.\n",
        "\n",
        "        Args:\n",
        "            start_node (str): The node where the walk starts.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of nodes representing the random walk.\n",
        "        \"\"\"\n",
        "        walk = [start_node]\n",
        "\n",
        "        # If the node has no neighbors, return the walk\n",
        "        if len(self.graph[start_node]) == 0:\n",
        "            return walk\n",
        "\n",
        "        # Start with a random first step\n",
        "        current_node = start_node\n",
        "        next_node = random.choice(self.graph[current_node])\n",
        "        walk.append(next_node)\n",
        "\n",
        "        for _ in range(self.walk_length - 1):\n",
        "            current_node = walk[-1]\n",
        "            prev_node = walk[-2] if len(walk) > 1 else None\n",
        "\n",
        "            neighbors = self.graph[current_node]\n",
        "            if len(neighbors) == 0:\n",
        "                break  # Stop if no outgoing edges\n",
        "\n",
        "            # Compute transition probabilities\n",
        "            probabilities = self.get_transition_probabilities(prev_node, current_node, neighbors)\n",
        "\n",
        "            # Choose next node based on transition probabilities\n",
        "            next_node = random.choices(neighbors, weights=probabilities)[0]\n",
        "            walk.append(next_node)\n",
        "\n",
        "        return walk\n",
        "\n",
        "    def get_transition_probabilities(self, prev_node, current_node, neighbors):\n",
        "        \"\"\"\n",
        "        Computes the transition probabilities for the next step.\n",
        "\n",
        "        Args:\n",
        "            prev_node (str): Previous node in the walk.\n",
        "            current_node (str): Current node in the walk.\n",
        "            neighbors (list): List of neighbor nodes.\n",
        "\n",
        "        Returns:\n",
        "            list: Transition probabilities for each neighbor.\n",
        "        \"\"\"\n",
        "        probabilities = []\n",
        "        for neighbor in neighbors:\n",
        "            if neighbor == prev_node:  # Returning to the previous node\n",
        "                probabilities.append(1 / self.p)\n",
        "            elif neighbor in self.graph[current_node]:  # Normal neighbor\n",
        "                probabilities.append(1)\n",
        "            else:  # Distant node\n",
        "                probabilities.append(1 / self.q)\n",
        "\n",
        "        return probabilities\n",
        "\n",
        "    def generate_walks(self):\n",
        "        \"\"\"\n",
        "        Generates multiple random walks for each node.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of all generated walks.\n",
        "        \"\"\"\n",
        "        walks = []\n",
        "        nodes = list(self.graph.keys())\n",
        "        for _ in range(self.num_walks):\n",
        "            random.shuffle(nodes)  # Shuffle nodes to reduce bias\n",
        "            for node in nodes:\n",
        "                walk = self.random_walk(node)\n",
        "                walks.append(walk)\n",
        "        return walks\n",
        "\n",
        "# Example Usage\n",
        "walker = Node2VecRandomWalker(graph, p=1.0, q=1.0, walk_length=10, num_walks=5)\n",
        "walks = walker.generate_walks()\n",
        "\n",
        "# Print a few random walks\n",
        "print(\"Sample Random Walks:\", walks[:3])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mH0gyqoo3Jsp",
        "outputId": "ac416faf-1ea2-4e49-e98c-66c5fbb2cae1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Random Walks: [['http://www.cs.cornell.edu/info/people/mishaal/home.html', 'http://www.cs.cornell.edu', 'http://cs-tr.cs.cornell.edu'], ['http://www.cs.utexas.edu/users/ejp', 'http://www.cs.utexas.edu/users/dmcl', 'http://www.cs.utexas.edu'], ['http://www.cs.wisc.edu/~cchin/cchin.html', 'http://www.cs.wisc.edu']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "\n",
        "class SkipGramDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, walks, window_size=2):\n",
        "        \"\"\"\n",
        "        Converts random walks into Skip-Gram training pairs.\n",
        "\n",
        "        Args:\n",
        "            walks (list of lists): List of random walks (sequences of nodes).\n",
        "            window_size (int): Context window size.\n",
        "        \"\"\"\n",
        "        self.window_size = window_size\n",
        "        self.word2idx = {}  # Node to index mapping\n",
        "        self.idx2word = []  # Index to node mapping\n",
        "        self.pairs = []  # Skip-Gram (center, context) pairs\n",
        "\n",
        "        # Assign unique indices to each node\n",
        "        self.build_vocab(walks)\n",
        "\n",
        "        # Extract Skip-Gram training pairs\n",
        "        self.generate_skipgram_pairs(walks)\n",
        "\n",
        "    def build_vocab(self, walks):\n",
        "        \"\"\"Creates node-to-index mappings.\"\"\"\n",
        "        unique_nodes = set(node for walk in walks for node in walk)\n",
        "        self.word2idx = {node: i for i, node in enumerate(unique_nodes)}\n",
        "        self.idx2word = list(unique_nodes)\n",
        "\n",
        "    def generate_skipgram_pairs(self, walks):\n",
        "        \"\"\"Generates (center, context) training pairs.\"\"\"\n",
        "        for walk in walks:\n",
        "            indexed_walk = [self.word2idx[node] for node in walk]\n",
        "            for i, center in enumerate(indexed_walk):\n",
        "                for j in range(-self.window_size, self.window_size + 1):\n",
        "                    context_idx = i + j\n",
        "                    if j != 0 and 0 <= context_idx < len(indexed_walk):\n",
        "                        self.pairs.append((center, indexed_walk[context_idx]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.pairs[idx], dtype=torch.long)\n",
        "\n",
        "# Example Usage\n",
        "window_size = 2\n",
        "dataset = SkipGramDataset(walks, window_size=window_size)\n",
        "\n",
        "# Print a few training pairs\n",
        "print(\"Sample Training Pairs (Node Indices):\", dataset.pairs[:5])\n",
        "print(\"Total training samples:\", len(dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3QoOMNn3Jvh",
        "outputId": "3aeeca49-2a8a-4207-f357-cd35b697465f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Training Pairs (Node Indices): [(14, 831), (14, 7), (831, 14), (831, 7), (7, 14)]\n",
            "Total training samples: 27052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class SkipGramModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        \"\"\"\n",
        "        Initializes the Skip-Gram model with two embedding layers.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Total number of unique nodes.\n",
        "            embedding_dim (int): Size of the embedding vectors.\n",
        "        \"\"\"\n",
        "        super(SkipGramModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)  # Center node embeddings\n",
        "        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)  # Context node embeddings\n",
        "\n",
        "        nn.init.xavier_uniform_(self.embeddings.weight)\n",
        "        nn.init.xavier_uniform_(self.context_embeddings.weight)\n",
        "\n",
        "    def forward(self, center_nodes, context_nodes, negative_samples):\n",
        "        \"\"\"\n",
        "        Computes Skip-Gram loss with negative sampling.\n",
        "\n",
        "        Args:\n",
        "            center_nodes (Tensor): Indices of center nodes.\n",
        "            context_nodes (Tensor): Indices of positive context nodes.\n",
        "            negative_samples (Tensor): Indices of negative samples.\n",
        "\n",
        "        Returns:\n",
        "            loss (Tensor): Computed loss value.\n",
        "        \"\"\"\n",
        "        center_emb = self.embeddings(center_nodes)  # Shape: (batch_size, embedding_dim)\n",
        "        context_emb = self.context_embeddings(context_nodes)  # Shape: (batch_size, embedding_dim)\n",
        "        negative_emb = self.context_embeddings(negative_samples)  # Shape: (batch_size, num_neg_samples, embedding_dim)\n",
        "\n",
        "        # Compute positive similarity (dot product)\n",
        "        positive_score = torch.mul(center_emb, context_emb).sum(dim=1)  # (batch_size)\n",
        "        positive_loss = torch.log(torch.sigmoid(positive_score))\n",
        "\n",
        "        # Compute negative similarity (dot product with negative samples)\n",
        "        negative_score = torch.bmm(negative_emb, center_emb.unsqueeze(2)).squeeze(2)  # (batch_size, num_neg_samples)\n",
        "        negative_loss = torch.log(torch.sigmoid(-negative_score)).sum(dim=1)  # Sum over negative samples\n",
        "\n",
        "        # Compute total loss (maximize positive, minimize negative)\n",
        "        loss = -torch.mean(positive_loss + negative_loss)\n",
        "        return loss\n",
        "\n",
        "# Example Usage\n",
        "vocab_size = len(dataset.word2idx)\n",
        "embedding_dim = 256\n",
        "model = SkipGramModel(vocab_size, embedding_dim)\n",
        "\n",
        "# Print model summary\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mv2r-0mJ3JyL",
        "outputId": "08c39ac8-8d35-4194-a823-1a165258e25e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SkipGramModel(\n",
            "  (embeddings): Embedding(877, 256)\n",
            "  (context_embeddings): Embedding(877, 256)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "class Node2VecTrainer:\n",
        "    def __init__(self, model, dataset, num_neg_samples=5, batch_size=1024, lr=0.001, epochs=30):\n",
        "        \"\"\"\n",
        "        Initializes the trainer for Skip-Gram with negative sampling.\n",
        "\n",
        "        Args:\n",
        "            model (SkipGramModel): The Skip-Gram neural network.\n",
        "            dataset (SkipGramDataset): Dataset containing training pairs.\n",
        "            num_neg_samples (int): Number of negative samples per positive pair.\n",
        "            batch_size (int): Number of samples per batch.\n",
        "            lr (float): Learning rate.\n",
        "            epochs (int): Number of training epochs.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.dataset = dataset\n",
        "        self.num_neg_samples = num_neg_samples\n",
        "        self.batch_size = batch_size\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Create DataLoader for batching\n",
        "        self.dataloader = torch.utils.data.DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "\n",
        "    def get_negative_samples(self, batch_size):\n",
        "        \"\"\"Samples negative nodes randomly.\"\"\"\n",
        "        vocab_size = len(self.dataset.word2idx)\n",
        "        return torch.randint(0, vocab_size, (batch_size, self.num_neg_samples), device=self.device)\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Trains the model using Skip-Gram with negative sampling.\"\"\"\n",
        "        self.model.train()\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            total_loss = 0\n",
        "            for batch in self.dataloader:\n",
        "                center_nodes, context_nodes = batch[:, 0].to(self.device), batch[:, 1].to(self.device)\n",
        "                negative_samples = self.get_negative_samples(center_nodes.shape[0])\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss = self.model(center_nodes, context_nodes, negative_samples)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{self.epochs}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "    def get_embedding(self, node):\n",
        "        \"\"\"Retrieves the learned embedding for a node.\"\"\"\n",
        "        node_idx = self.dataset.word2idx.get(node)\n",
        "        if node_idx is None:\n",
        "            return None\n",
        "        return self.model.embeddings.weight[node_idx].detach().cpu().numpy()\n",
        "\n",
        "# Initialize Trainer and Train Model\n",
        "trainer = Node2VecTrainer(model, dataset, epochs=40, batch_size=1024)\n",
        "trainer.train()\n",
        "\n",
        "# Save embeddings\n",
        "embeddings = {node: trainer.get_embedding(node) for node in dataset.word2idx}\n",
        "np.save(\"webkb_node2vec_embeddings.npy\", embeddings)\n",
        "print(\"Node2Vec embeddings saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5dsiwHe3J00",
        "outputId": "e384db17-737f-46f1-c8e3-c1cab5ab3bf9"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40, Loss: 110.8408\n",
            "Epoch 2/40, Loss: 106.3209\n",
            "Epoch 3/40, Loss: 94.9075\n",
            "Epoch 4/40, Loss: 72.8846\n",
            "Epoch 5/40, Loss: 51.9389\n",
            "Epoch 6/40, Loss: 40.8168\n",
            "Epoch 7/40, Loss: 34.5666\n",
            "Epoch 8/40, Loss: 30.2112\n",
            "Epoch 9/40, Loss: 26.6790\n",
            "Epoch 10/40, Loss: 23.5658\n",
            "Epoch 11/40, Loss: 20.9984\n",
            "Epoch 12/40, Loss: 18.8694\n",
            "Epoch 13/40, Loss: 17.0663\n",
            "Epoch 14/40, Loss: 15.3359\n",
            "Epoch 15/40, Loss: 14.1066\n",
            "Epoch 16/40, Loss: 12.9002\n",
            "Epoch 17/40, Loss: 12.0654\n",
            "Epoch 18/40, Loss: 11.3709\n",
            "Epoch 19/40, Loss: 10.6102\n",
            "Epoch 20/40, Loss: 10.0513\n",
            "Epoch 21/40, Loss: 9.5741\n",
            "Epoch 22/40, Loss: 9.2907\n",
            "Epoch 23/40, Loss: 8.9640\n",
            "Epoch 24/40, Loss: 8.3541\n",
            "Epoch 25/40, Loss: 8.1977\n",
            "Epoch 26/40, Loss: 7.9861\n",
            "Epoch 27/40, Loss: 7.7645\n",
            "Epoch 28/40, Loss: 7.5848\n",
            "Epoch 29/40, Loss: 7.2982\n",
            "Epoch 30/40, Loss: 7.3052\n",
            "Epoch 31/40, Loss: 6.7735\n",
            "Epoch 32/40, Loss: 6.8254\n",
            "Epoch 33/40, Loss: 6.6645\n",
            "Epoch 34/40, Loss: 6.6863\n",
            "Epoch 35/40, Loss: 6.5669\n",
            "Epoch 36/40, Loss: 6.3147\n",
            "Epoch 37/40, Loss: 6.3835\n",
            "Epoch 38/40, Loss: 6.3368\n",
            "Epoch 39/40, Loss: 6.3722\n",
            "Epoch 40/40, Loss: 6.1883\n",
            "Node2Vec embeddings saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Extract train and test nodes from the cites files\n",
        "def get_nodes_from_cites(file_path):\n",
        "    nodes = set()\n",
        "    with open(file_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            cited, citing = line.strip().split()\n",
        "            nodes.add(cited)\n",
        "            nodes.add(citing)\n",
        "    return nodes\n",
        "\n",
        "# Load train and test nodes\n",
        "train_nodes = get_nodes_from_cites(\"webkb_combined/combined_train.cites\")\n",
        "test_nodes = get_nodes_from_cites(\"webkb_combined/combined_test.cites\")\n",
        "\n",
        "# Filter nodes that exist in embeddings\n",
        "train_nodes = [node for node in train_nodes if node in embeddings]\n",
        "test_nodes = [node for node in test_nodes if node in embeddings]\n",
        "\n",
        "# Prepare feature matrices\n",
        "X_train = np.array([embeddings[node] for node in train_nodes])\n",
        "y_train = np.array([labels[node] for node in train_nodes])\n",
        "\n",
        "X_test = np.array([embeddings[node] for node in test_nodes])\n",
        "y_test = np.array([labels[node] for node in test_nodes])\n",
        "\n",
        "# Train logistic regression\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(\"✅ Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"✅ Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeXIh98D6nP9",
        "outputId": "b9e5f43d-0e83-4f3e-c5ed-a7db57e2e9b3"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Accuracy: 0.7114624505928854\n",
            "✅ Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      course       0.76      0.67      0.71       124\n",
            "     faculty       0.92      0.44      0.60        77\n",
            "     project       0.85      0.49      0.62        57\n",
            "       staff       1.00      0.25      0.40        20\n",
            "     student       0.65      0.92      0.76       228\n",
            "\n",
            "    accuracy                           0.71       506\n",
            "   macro avg       0.84      0.55      0.62       506\n",
            "weighted avg       0.76      0.71      0.70       506\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2XRnUmj-6nVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6U33TsgX6nYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q9zaqFLw6nad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ITPkPjg6ndV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qNbQnq0s6ngG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EJMOyolb6njk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}