{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvXqtosOFk6OnyEoOzIPPp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arumishra/Assignment-Codes/blob/main/complexnetworks2_2GCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zBFmlhxeVaHR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71b4582d-4ae5-4ba8-9933-39ce363eb431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted files: ['README', 'wisconsin.cites', 'washington.content', 'wisconsin.content', 'texas.cites', 'cornell.content', 'washington.cites', 'texas.content', 'cornell.cites']\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/webkb.zip\"  # Path to the ZIP file\n",
        "extract_path = \"/content\"  # Extract directly to /content\n",
        "\n",
        "# Extract the dataset\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# Set the correct data directory path\n",
        "data_directory = \"/content/webkb\"  # Now this should directly contain the files\n",
        "\n",
        "# List extracted files to verify\n",
        "print(\"Extracted files:\", os.listdir(data_directory))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define paths\n",
        "data_directory = \"/content/webkb\"\n",
        "output_cites = \"/content/webkb_combined/combined.cites\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(\"/content/webkb_combined\", exist_ok=True)\n",
        "\n",
        "# Merge .cites files\n",
        "with open(output_cites, \"w\") as outfile:\n",
        "    for file in os.listdir(data_directory):\n",
        "        if file.endswith(\".cites\"):\n",
        "            file_path = os.path.join(data_directory, file)\n",
        "            print(f\"🔄 Merging {file_path}\")\n",
        "            with open(file_path, \"r\") as infile:\n",
        "                lines = infile.readlines()\n",
        "                if lines:\n",
        "                    print(f\"✅ {file} has {len(lines)} edges\")\n",
        "                    outfile.writelines(lines)\n",
        "                else:\n",
        "                    print(f\"⚠️ {file} is empty!\")\n",
        "\n",
        "print(\"✅ Merging .cites files complete!\")\n"
      ],
      "metadata": {
        "id": "A_epow4dVefx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94246062-c16b-41c7-d612-9b6748708436"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Merging /content/webkb/wisconsin.cites\n",
            "✅ wisconsin.cites has 530 edges\n",
            "🔄 Merging /content/webkb/texas.cites\n",
            "✅ texas.cites has 328 edges\n",
            "🔄 Merging /content/webkb/washington.cites\n",
            "✅ washington.cites has 446 edges\n",
            "🔄 Merging /content/webkb/cornell.cites\n",
            "✅ cornell.cites has 304 edges\n",
            "✅ Merging .cites files complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define output file path\n",
        "output_content = \"/content/webkb_combined/combined.content\"\n",
        "\n",
        "# Merge .content files\n",
        "with open(output_content, \"w\") as outfile:\n",
        "    for file in os.listdir(data_directory):\n",
        "        if file.endswith(\".content\"):\n",
        "            file_path = os.path.join(data_directory, file)\n",
        "            print(f\"🔄 Merging {file_path}\")\n",
        "            with open(file_path, \"r\") as infile:\n",
        "                lines = infile.readlines()\n",
        "                if lines:\n",
        "                    print(f\"✅ {file} has {len(lines)} nodes\")\n",
        "                    outfile.writelines(lines)\n",
        "                else:\n",
        "                    print(f\"⚠️ {file} is empty!\")\n",
        "\n",
        "print(\"✅ Merging .content files complete!\")\n"
      ],
      "metadata": {
        "id": "7EiCThhtVeiZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4104205-67c9-4889-f15d-86a508eebc15"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Merging /content/webkb/washington.content\n",
            "✅ washington.content has 230 nodes\n",
            "🔄 Merging /content/webkb/wisconsin.content\n",
            "✅ wisconsin.content has 265 nodes\n",
            "🔄 Merging /content/webkb/cornell.content\n",
            "✅ cornell.content has 195 nodes\n",
            "🔄 Merging /content/webkb/texas.content\n",
            "✅ texas.content has 187 nodes\n",
            "✅ Merging .content files complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"✅ Size of combined.cites:\", os.path.getsize(output_cites), \"bytes\")\n",
        "print(\"✅ Size of combined.content:\", os.path.getsize(output_content), \"bytes\")\n"
      ],
      "metadata": {
        "id": "10BUwrgWVelI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a61f088-f1a0-4b4c-bd80-c05f26a6f045"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Size of combined.cites: 137522 bytes\n",
            "✅ Size of combined.content: 3034502 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# ------------------- Activation and Utility -------------------\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "# ------------------- Adam Optimizer -------------------\n",
        "class AdamOptimizer:\n",
        "    def __init__(self, shape, lr=0.01, beta1=0.9, beta2=0.999, eps=1e-8):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "        self.m = np.zeros(shape)\n",
        "        self.v = np.zeros(shape)\n",
        "        self.t = 0\n",
        "\n",
        "    def update(self, param, grad):\n",
        "        self.t += 1\n",
        "        self.m = self.beta1 * self.m + (1 - self.beta1) * grad\n",
        "        self.v = self.beta2 * self.v + (1 - self.beta2) * (grad ** 2)\n",
        "        m_hat = self.m / (1 - self.beta1 ** self.t)\n",
        "        v_hat = self.v / (1 - self.beta2 ** self.t)\n",
        "        param -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
        "        return param\n",
        "\n",
        "# ------------------- GCN Layer -------------------\n",
        "class GCNLayer:\n",
        "    def __init__(self, in_dim, out_dim, lr=0.01):\n",
        "        self.W = np.random.randn(in_dim, out_dim) * 0.01\n",
        "        self.optimizer = AdamOptimizer(self.W.shape, lr)\n",
        "\n",
        "    def forward(self, X, A_hat):\n",
        "        self.X = X\n",
        "        self.Z = A_hat @ X @ self.W\n",
        "        return relu(self.Z)\n",
        "\n",
        "    def backward(self, grad, A_hat):\n",
        "        grad_Z = grad * (self.Z > 0).astype(float)\n",
        "        dW = self.X.T @ A_hat.T @ grad_Z\n",
        "        self.W = self.optimizer.update(self.W, dW)\n",
        "        return grad_Z @ self.W.T\n",
        "\n",
        "# ------------------- GCN Model -------------------\n",
        "class GCN:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, lr=0.01):\n",
        "        self.gcn1 = GCNLayer(input_dim, hidden_dim, lr)\n",
        "        self.gcn2 = GCNLayer(hidden_dim, output_dim, lr)\n",
        "\n",
        "    def forward(self, X, A_hat, training=True):\n",
        "        self.h1 = self.gcn1.forward(X, A_hat)\n",
        "        if training:\n",
        "            dropout_mask = (np.random.rand(*self.h1.shape) > 0.5)\n",
        "            self.h1 *= dropout_mask  # Dropout with rate = 0.5\n",
        "        self.out = A_hat @ self.h1 @ self.gcn2.W\n",
        "        return softmax(self.out)\n",
        "\n",
        "    def backward(self, X, A_hat, y_true, idx_train):\n",
        "        preds = softmax(self.out)\n",
        "        y_onehot = np.eye(preds.shape[1])[y_true]\n",
        "        loss_grad = (preds - y_onehot) / len(idx_train)\n",
        "        grad_out = np.zeros_like(loss_grad)\n",
        "        grad_out[idx_train] = loss_grad[idx_train]\n",
        "\n",
        "        grad_h1 = grad_out @ self.gcn2.W.T\n",
        "        self.gcn2.W = self.gcn2.optimizer.update(self.gcn2.W, self.h1.T @ A_hat.T @ grad_out)\n",
        "        self.gcn1.backward(grad_h1, A_hat)\n",
        "\n",
        "    def predict(self):\n",
        "        return np.argmax(self.out, axis=1)\n",
        "\n",
        "# ------------------- Data Processing -------------------\n",
        "def load_content_file(path):\n",
        "    paper_ids, features, labels = [], [], []\n",
        "\n",
        "    with open(path, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            paper_ids.append(parts[0])\n",
        "            features.append(list(map(int, parts[1:-1])))\n",
        "            labels.append(parts[-1])\n",
        "\n",
        "    label_set = sorted(set(labels))\n",
        "    label_to_index = {label: i for i, label in enumerate(label_set)}\n",
        "    encoded_labels = np.array([label_to_index[label] for label in labels])\n",
        "\n",
        "    id_to_index = {pid: i for i, pid in enumerate(paper_ids)}\n",
        "    features = np.array(features)\n",
        "    return features, encoded_labels, id_to_index, label_to_index\n",
        "\n",
        "def load_cites_file(path, id_to_index, num_nodes):\n",
        "    adj = np.zeros((num_nodes, num_nodes))\n",
        "    with open(path, 'r') as f:\n",
        "        for line in f:\n",
        "            src, dst = line.strip().split()\n",
        "            if src in id_to_index and dst in id_to_index:\n",
        "                i, j = id_to_index[src], id_to_index[dst]\n",
        "                adj[i][j] = 1\n",
        "                adj[j][i] = 1\n",
        "    return adj\n",
        "\n",
        "def normalize_adjacency(adj):\n",
        "    adj = adj + np.eye(adj.shape[0])\n",
        "    deg = np.sum(adj, axis=1)\n",
        "    d_inv_sqrt = np.power(deg, -0.5)\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    D_inv_sqrt = np.diag(d_inv_sqrt)\n",
        "    return D_inv_sqrt @ adj @ D_inv_sqrt\n",
        "\n",
        "def split_nodes(num_nodes, train_ratio=0.7):\n",
        "    indices = np.arange(num_nodes)\n",
        "    np.random.shuffle(indices)\n",
        "    train_end = int(train_ratio * num_nodes)\n",
        "    return indices[:train_end], indices[train_end:]\n",
        "\n",
        "def cross_entropy(preds, labels, idx):\n",
        "    logp = -np.log(preds[np.arange(len(preds)), labels])\n",
        "    return np.mean(logp[idx])\n",
        "\n",
        "# ------------------- Training and Evaluation -------------------\n",
        "def train_gcn(X, y, A_hat, idx_train, idx_test, num_classes, epochs=200, lr=0.01):\n",
        "    model = GCN(X.shape[1], 16, num_classes, lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        out = model.forward(X, A_hat)\n",
        "        loss = cross_entropy(out, y, idx_train)\n",
        "        model.backward(X, A_hat, y, idx_train)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            pred = model.predict()\n",
        "            acc = accuracy_score(y[idx_train], pred[idx_train])\n",
        "            print(f\"Epoch {epoch} | Loss: {loss:.4f} | Train Acc: {acc:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def evaluate(model, y_true, idx_test):\n",
        "    pred = model.predict()\n",
        "    y_pred = pred[idx_test]\n",
        "    y_true = y_true[idx_test]\n",
        "    print(\"\\n--- Test Set Evaluation ---\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_true, y_pred, average='macro'))\n",
        "    print(\"Recall:\", recall_score(y_true, y_pred, average='macro'))\n",
        "    print(\"F1 Score:\", f1_score(y_true, y_pred, average='macro'))\n",
        "    import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "def evaluate_detailed(model, y_true, idx_test, label_map, paper_ids):\n",
        "    pred = model.predict()\n",
        "    y_pred = pred[idx_test]\n",
        "    y_true = y_true[idx_test]\n",
        "\n",
        "    # --- Metric Summary ---\n",
        "    print(\"\\n--- Per-Class Metrics ---\")\n",
        "    print(classification_report(y_true, y_pred, target_names=list(label_map.keys())))\n",
        "\n",
        "    # --- Confusion Matrix ---\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=list(label_map.keys()),\n",
        "                yticklabels=list(label_map.keys()))\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ------------------- Main -------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    X, y, id_to_idx, label_map = load_content_file(\"webkb_combined/combined.content\")\n",
        "    A = load_cites_file(\"webkb_combined/combined.cites\", id_to_idx, X.shape[0])\n",
        "    A_hat = normalize_adjacency(A)\n",
        "\n",
        "    # Train-test split\n",
        "    idx_train, idx_test = split_nodes(len(y), 0.7)\n",
        "\n",
        "    # Train\n",
        "    model = train_gcn(X, y, A_hat, idx_train, idx_test, num_classes=len(label_map))\n",
        "\n",
        "    # Evaluate\n",
        "    evaluate(model, y, idx_test)\n"
      ],
      "metadata": {
        "id": "B_N0nEi-VeqQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52807072-cc04-4615-e455-f763840f266c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Loss: 1.6096 | Train Acc: 0.1077\n",
            "Epoch 10 | Loss: 1.3552 | Train Acc: 0.4796\n",
            "Epoch 20 | Loss: 1.1826 | Train Acc: 0.5530\n",
            "Epoch 30 | Loss: 1.0459 | Train Acc: 0.6085\n",
            "Epoch 40 | Loss: 0.9824 | Train Acc: 0.6281\n",
            "Epoch 50 | Loss: 0.9508 | Train Acc: 0.6248\n",
            "Epoch 60 | Loss: 0.9983 | Train Acc: 0.5824\n",
            "Epoch 70 | Loss: 0.9558 | Train Acc: 0.6264\n",
            "Epoch 80 | Loss: 0.9574 | Train Acc: 0.6020\n",
            "Epoch 90 | Loss: 0.9768 | Train Acc: 0.6003\n",
            "Epoch 100 | Loss: 0.9751 | Train Acc: 0.5938\n",
            "Epoch 110 | Loss: 0.9449 | Train Acc: 0.6362\n",
            "Epoch 120 | Loss: 0.9403 | Train Acc: 0.5938\n",
            "Epoch 130 | Loss: 0.9474 | Train Acc: 0.6378\n",
            "Epoch 140 | Loss: 0.9821 | Train Acc: 0.5840\n",
            "Epoch 150 | Loss: 0.9307 | Train Acc: 0.6297\n",
            "Epoch 160 | Loss: 0.9384 | Train Acc: 0.6232\n",
            "Epoch 170 | Loss: 0.9916 | Train Acc: 0.5987\n",
            "Epoch 180 | Loss: 0.9651 | Train Acc: 0.5971\n",
            "Epoch 190 | Loss: 0.9164 | Train Acc: 0.6052\n",
            "\n",
            "--- Test Set Evaluation ---\n",
            "Accuracy: 0.4128787878787879\n",
            "Precision: 0.2829938532348518\n",
            "Recall: 0.27390353753990115\n",
            "F1 Score: 0.2720558639599119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "def evaluate_detailed(model, y_true, idx_test, label_map, paper_ids):\n",
        "    pred = model.predict()\n",
        "    y_pred = pred[idx_test]\n",
        "    y_true = y_true[idx_test]\n",
        "\n",
        "    # --- Metric Summary ---\n",
        "    print(\"\\n--- Per-Class Metrics ---\")\n",
        "    print(classification_report(y_true, y_pred, target_names=list(label_map.keys())))\n",
        "\n",
        "    # --- Confusion Matrix ---\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=list(label_map.keys()),\n",
        "                yticklabels=list(label_map.keys()))\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "gNcnJoDrVetC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RrZtbFfnVev2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fiG2hUh_Veyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LW7zNuyvVe1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Alczzl5dVe3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "77XcJbyEVe6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7z1iwpZ4Ve9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UHaLp_joVe__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g89ee5IFVfCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ihm7GiiRVfFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1fLbS6ovVfIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DRiEKYcXVfL1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}